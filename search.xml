<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>伪代码</title>
      <link href="/posts/ce589481/"/>
      <url>/posts/ce589481/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>伪代码(Pseudocode)是一种算法描述语言。使用伪代码的目的是为了使被描述的算法可以容易地以任何一种编程语言(C, Java, Pascal)实现。因此，伪代码必须结构清晰，代码简单，可读性好，并且类似自然语言。</p><p><strong>伪代码的优点</strong></p><ul><li><p>提高任何方法的可读性。这是开始实现算法的最佳方法之一。</p></li><li><p>充当程序与算法或流程图之间的桥梁。也可以作为一个粗略的文档，因此当写出伪代码时，可以很容易地理解一个开发人员的程序。在行业中，文档是必不可少的。这就是证明伪代码至关重要的地方。</p></li><li><p>伪代码的主要目标是解释程序的每一行应该做什么，从而使程序员更容易构建代码构建阶段。</p></li></ul><h1 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h1><ul><li>在伪代码中，<strong>每一条指令占一行</strong>(else if 例外)，指令后不跟任何符号；</li><li><strong>“缩进”</strong> 表示程序中的分支程序结构（同一模块的语句有相同的缩进量，次一级模块的语句相对与其父级模块的语句缩进）；</li><li>通常每个算法开始时都要描述它的输入和输出，而且算法中的每一行都给编上行号，在解释算法的过程中会经常使用算法步骤中的行号来指代算法的步骤；</li><li>每一行可以加上编号（也可不加）。</li></ul><h2 id="变量的声明"><a href="#变量的声明" class="headerlink" title="变量的声明"></a>变量的声明</h2><p>算法中出现的数组、变量可以是：整数、实数、字符、字符串、指针。在注释中给出定义。</p><h2 id="指令的表示"><a href="#指令的表示" class="headerlink" title="指令的表示"></a>指令的表示</h2><p>在</p><hr><blockquote><p>参考<br><a href="https://www.cnblogs.com/linuxAndMcu/p/11242905.html">https://www.cnblogs.com/linuxAndMcu/p/11242905.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KNN</title>
      <link href="/posts/7c10b52f/"/>
      <url>/posts/7c10b52f/</url>
      
        <content type="html"><![CDATA[<h1 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h1><p>KNN（K-Nearest Neighbors，K 近邻算法）是一种 <strong>基于实例（Instance-based）</strong> 的监督学习方法，其核心思想可以概括为：<br><strong>“一个样本的类别由与它最相似的 K 个样本共同决定。”</strong></p><h2 id="物以类聚"><a href="#物以类聚" class="headerlink" title="物以类聚"></a>物以类聚</h2><p>KNN 的思想来源于人类日常生活中的直觉判断，即 <strong>“物以类聚”</strong>。</p><p>例如，在一个学生成绩数据集中：</p><ul><li>如果某个新学生的成绩与多数“优秀”学生非常接近，</li><li>那么我们有理由认为该学生也属于“优秀”类别。</li></ul><p>在 KNN 中，这种“接近”通过<strong>距离度量</strong>来进行量化。</p><h2 id="基本假设"><a href="#基本假设" class="headerlink" title="基本假设"></a>基本假设</h2><p>隐含了一个重要假设：<br><strong>在特征空间中，距离相近的样本具有相似的属性或类别。</strong></p><p>即：</p><ul><li>特征空间中的 <strong>局部结构</strong> 比全局模型更重要；</li><li>不同类别间在空间上应当具有一定的可分性。</li></ul><h2 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h2><script type="math/tex; mode=display">\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}</script><ul><li>$x_i \in \mathbb{R}^d$ 表示第 $i$ 个样本的特征向量；</li><li>$y_i$ 表示对应的类别标签或数值输出。</li></ul><p>对于一个待分类（或回归）的新样本 $x$，KNN 的基本步骤如下：</p><ol><li>计算样本 $x$ 与训练集中所有样本之间的距离；</li><li>按距离从小到大排序，选取距离最近的 $K$ 个样本；</li><li>根据这 $K$ 个邻居的标签对 $x$ 进行预测。</li></ol><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>KNN 算法是一种<strong>懒惰学习（Lazy Learning）</strong>方法，其训练阶段几乎不进行任何计算，主要计算集中在预测阶段。</p><p>对于一个待预测样本 $x$，KNN 的基本流程如下：</p><ol><li>计算 $x$ 与训练集中所有样本之间的距离；</li><li>按距离从小到大对样本进行排序；</li><li>选取距离最近的 $K$ 个样本作为近邻；</li><li>根据任务类型（分类或回归）输出预测结果。</li></ol><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><hr><h2 id="Algorithm-1-K-Nearest-Neighbors-KNN"><a href="#Algorithm-1-K-Nearest-Neighbors-KNN" class="headerlink" title="Algorithm 1: K-Nearest Neighbors (KNN)"></a><strong>Algorithm 1</strong>: K-Nearest Neighbors (KNN)</h2><p><strong>Input</strong>:  </p><ul><li>Training dataset $\mathcal{D} = {(x_1, y_1), \dots, (x_n, y_n)}$  </li><li>Query sample $x$  </li><li>Number of neighbors $K$  </li><li>Distance metric $d(\cdot, \cdot)$  </li></ul><p><strong>Output</strong>:  </p><h2 id="Predicted-label-hat-y"><a href="#Predicted-label-hat-y" class="headerlink" title="- Predicted label $\hat{y}$"></a>- Predicted label $\hat{y}$</h2><p>1:  Initialize distance list $D = \emptyset$;<br>2:  <strong>for</strong> $i = 1$ <strong>to</strong> $n$ <strong>do</strong><br>3:  Compute distance $d_i = d(x, x_i)$;<br>4:  Append $(d_i, y_i)$ to $D$;<br>5:  <strong>end for</strong>  </p><p>6:  Sort $D$ in ascending order according to $d_i$;<br>7:  Select the first $K$ elements from $D$ as $\mathcal{N}_K(x)$;  </p><p>8:  <strong>if</strong> task is classification <strong>then</strong><br>9:  $\hat{y} \leftarrow$ majority vote of labels in $ \mathcal{N}_K(x) $ ;</p><p>10: <strong>else if</strong> task is regression <strong>then</strong>  </p><p>11:  $ \hat{y} \leftarrow \frac{1}{K} \sum_{(d_i, y_i) \in \mathcal{N}_K(x)} y_i $ ;</p><p>12: <strong>end if</strong>  </p><p>13: <strong>return</strong> $\hat{y}$;</p><h2 id="距离度量方法"><a href="#距离度量方法" class="headerlink" title="距离度量方法"></a>距离度量方法</h2><p>在 KNN 算法中，“相似度”的判断完全依赖于<strong>距离度量函数</strong>。<br>不同的距离度量方式会直接影响邻居的选取，从而影响最终分类或回归结果。</p><p>设两个样本点为<br>$x = (x_1, x_2, \dots, x_d)$，<br>$y = (y_1, y_2, \dots, y_d)$，<br>其中 $d$ 表示特征维度。</p><h3 id="欧式距离（Euclidean-Distance）"><a href="#欧式距离（Euclidean-Distance）" class="headerlink" title="欧式距离（Euclidean Distance）"></a>欧式距离（Euclidean Distance）</h3><p>欧氏距离是最常用、最直观的一种距离度量方式，表示两点在欧几里得空间中的直线距离。</p><script type="math/tex; mode=display">d(x, y) = \sqrt{\sum_{i=1}^{d} (x_i - y_i)^2}</script><p><strong>特点：</strong></p><ul><li>适用于连续数值型特征；</li><li>对特征尺度敏感，常需进行归一化或标准化；</li><li>在低维空间中效果较好。</li></ul><h3 id="曼哈顿距离（Manhattan-Distance）"><a href="#曼哈顿距离（Manhattan-Distance）" class="headerlink" title="曼哈顿距离（Manhattan Distance）"></a>曼哈顿距离（Manhattan Distance）</h3><p>曼哈顿距离又称为 $L_1$ 距离，表示在坐标轴方向上的距离总和。</p><script type="math/tex; mode=display">d(x, y) = \sum_{i=1}^{d} |x_i - y_i|</script><p><strong>特点：</strong></p><ul><li>对异常值相对不如欧氏距离敏感；</li><li>适用于高维或稀疏特征空间；</li><li>在某些实际应用中比欧氏距离更稳定。</li></ul><h3 id="闵可夫斯基距离（Minkowski-Distance）"><a href="#闵可夫斯基距离（Minkowski-Distance）" class="headerlink" title="闵可夫斯基距离（Minkowski Distance）"></a>闵可夫斯基距离（Minkowski Distance）</h3><p>闵可夫斯基距离是欧氏距离和曼哈顿距离的统一形式，其定义如下：</p><script type="math/tex; mode=display">d(x, y) = \left( \sum_{i=1}^{d} |x_i - y_i|^p \right)^{\frac{1}{p}}, \quad p \geq 1</script><p>当：</p><ul><li>$p = 1$ 时，为曼哈顿距离；</li><li>$p = 2$ 时，为欧氏距离；</li><li>$p \to \infty$ 时，为切比雪夫距离。</li></ul><p><strong>特点：</strong></p><ul><li>提供了一种灵活的距离度量框架；</li><li>可根据具体问题选择合适的 $p$ 值。</li></ul><h3 id="余弦相似度（Cosine-Similarity）"><a href="#余弦相似度（Cosine-Similarity）" class="headerlink" title="余弦相似度（Cosine Similarity）"></a>余弦相似度（Cosine Similarity）</h3><p>余弦相似度衡量的是两个向量在方向上的相似性，而非距离大小，常用于文本、推荐系统等领域。</p><script type="math/tex; mode=display">\text{cos}(x, y) = \frac{x \cdot y}{\|x\| \|y\|}</script><p>其中，$x \cdot y$ 表示向量点积，$|x|$ 表示向量的 $L_2$ 范数。</p><p><strong>特点：</strong></p><ul><li>与向量的模长无关；</li><li>更关注特征的方向信息；</li><li>适合高维稀疏数据。</li></ul><h2 id="参数影响"><a href="#参数影响" class="headerlink" title="参数影响"></a>参数影响</h2><p>KNN 算法的性能对参数设置高度敏感，主要包括 <strong>K 值的选择</strong> 和 <strong>距离度量方式</strong>。</p><h3 id="K值"><a href="#K值" class="headerlink" title="K值"></a>K值</h3><p>$K$ 是 KNN 中最关键的超参数，其取值直接影响模型的偏差与方差。</p><ul><li><p><strong>K 较小（如 $K=1$）</strong>：</p><ul><li>模型对噪声敏感；</li><li>决策边界复杂；</li><li>容易发生过拟合（High Variance）。</li></ul></li><li><p><strong>K 较大</strong>：</p><ul><li>决策边界更平滑；</li><li>可能忽略局部结构；</li><li>容易发生欠拟合（High Bias）。</li></ul></li></ul><p>通常需要通过交叉验证等方法选择合适的 $K$ 值。</p><h3 id="距离度量方式的影响"><a href="#距离度量方式的影响" class="headerlink" title="距离度量方式的影响"></a>距离度量方式的影响</h3><p>不同的距离度量会导致不同的“近邻”集合：</p><ul><li>欧氏距离对特征尺度敏感，需进行特征归一化；</li><li>曼哈顿距离在高维空间中更具鲁棒性；</li><li>余弦相似度更关注方向相似性而非数值大小。</li></ul><p>因此，在实际应用中应根据数据分布和任务特点选择合适的距离度量方式。</p><h3 id="特征尺度的影响"><a href="#特征尺度的影响" class="headerlink" title="特征尺度的影响"></a>特征尺度的影响</h3><p>若各特征量纲差异较大，将导致某些特征主导距离计算结果。<br>常见的解决方法包括：</p><ul><li>Min-Max 归一化；</li><li>Z-score 标准化。</li></ul><p>特征预处理是保证 KNN 算法性能的重要前提。</p><h1 id="KNN优缺点"><a href="#KNN优缺点" class="headerlink" title="KNN优缺点"></a>KNN优缺点</h1>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OTSU</title>
      <link href="/posts/311baf7/"/>
      <url>/posts/311baf7/</url>
      
        <content type="html"><![CDATA[<h1 id="OTSU"><a href="#OTSU" class="headerlink" title="OTSU"></a>OTSU</h1><blockquote><p>论文：<a href="https://ieeexplore.ieee.org/document/4310076">A Threshold Selection Method from Gray-Level Histograms</a></p></blockquote><p>OTSU算法也称<strong>最大类间差法</strong>，有时也称之为大津算法，由大津于1979年提出，被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响，因此在数字图像处理上得到了广泛的应用。它是按图像的灰度分布特性,将图像分成背景(background)和目标(object)两部分。考虑到方差是灰度分布均匀性的一种度量,理想情况下，对于同一类，其类内方差应该是很小的，同时背景和目标之间的类间方差越大,说明构成图像的两部分的差别越大,当部分目标错分为背景或部分背景错分为目标都会导致两部分差别变小。因此,使类间方差最大的分割意味着错分概率最小。</p><p>该方法的基本思想是根据选取的阈值将图像分为目标和背景两个部分，<strong>通过最大化类间方差（最小化类内方差），自动确定最佳分割阈值</strong></p><h1 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h1><p>假设一幅灰度图像，其灰度级为 $0 \sim L-1$。</p><p>当选择某一阈值 (T) 时，图像被分为两类：</p><ul><li><strong>前景（目标）</strong>：灰度 $0 \sim T$</li><li><strong>背景</strong>：灰度 $T+1 \sim L-1$</li></ul><p>OTSU 的目标是：<br><strong>找到一个阈值 (T)，使得前景与背景的区分度最大</strong></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="灰度直方图概率分布"><a href="#灰度直方图概率分布" class="headerlink" title="灰度直方图概率分布"></a>灰度直方图概率分布</h2><p>设图像共有 $N$ 个像素，灰度级为 $i$，则：<br>$$<br>p_i &#x3D; \frac{n_i}{N}<br>$$</p><ul><li>$n_i$：灰度为 $i$ 的像素数</li><li>$p_i$：灰度 $i$ 出现的概率</li></ul><h2 id="类概率"><a href="#类概率" class="headerlink" title="类概率"></a>类概率</h2><p>对于阈值$T$:</p><ul><li><p>前景概率：<br>$$<br>\omega_0(T) &#x3D; \sum_{i&#x3D;0}^{T} p_i<br>$$</p></li><li><p>背景概率：<br>$$<br>\omega_1(T) &#x3D; \sum_{i&#x3D;T+1}^{L-1} p_i<br>$$</p></li></ul><h2 id="类均值"><a href="#类均值" class="headerlink" title="类均值"></a>类均值</h2><ul><li><p>前景均值：<br>$$<br>\mu_0(T) &#x3D; \frac{1}{\omega_0} \sum_{i&#x3D;0}^{T} i p_i<br>$$</p></li><li><p>背景均值<br>$$<br>\mu_1(T) &#x3D; \frac{1}{\omega_1} \sum_{i&#x3D;T+1}^{L-1} i p_i<br>$$</p></li><li><p>全局均值<br>$$<br>\mu &#x3D; \sum_{i&#x3D;0}^{L-1} i p_i<br>$$</p></li></ul><h2 id="类间方差"><a href="#类间方差" class="headerlink" title="类间方差"></a>类间方差</h2><p>$$<br>\sigma_b^2(T) &#x3D; \omega_0 (\mu_0 - \mu)^2 + \omega_1 (\mu_1 - \mu)^2<br>$$<br>$$<br>&#x3D; \omega_0 \omega_1 (\mu_0 - \mu_1)^2<br>$$</p><h2 id="最优阈值选择"><a href="#最优阈值选择" class="headerlink" title="最优阈值选择"></a>最优阈值选择</h2><p>$$<br>T^* &#x3D; \arg \max_T \sigma_b^2(T)<br>$$</p><p><strong>使类间方差最大的阈值即为 OTSU 阈值</strong></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><ol><li>计算图像灰度直方图</li><li>归一化为概率分布</li><li>遍历所有可能阈值 (T)</li><li>计算类概率、类均值、类间方差</li><li>选择使类间方差最大的阈值</li><li>根据该阈值进行二值化</li></ol><h1 id="底层代码"><a href="#底层代码" class="headerlink" title="底层代码"></a>底层代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">otsu_threshold</span>(<span class="params">image</span>):</span><br><span class="line">    <span class="comment"># 计算灰度直方图</span></span><br><span class="line">    hist = np.bincount(image.ravel(), minlength=<span class="number">256</span>)</span><br><span class="line">    total = image.size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    prob = hist / total</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全局均值</span></span><br><span class="line">    mu_total = np.<span class="built_in">sum</span>(np.arange(<span class="number">256</span>) * prob)</span><br><span class="line"></span><br><span class="line">    omega_0 = <span class="number">0.0</span></span><br><span class="line">    mu_0 = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    max_sigma = <span class="number">0.0</span></span><br><span class="line">    best_thresh = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        omega_0 += prob[t]</span><br><span class="line">        mu_0 += t * prob[t]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> omega_0 == <span class="number">0</span> <span class="keyword">or</span> omega_0 == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        omega_1 = <span class="number">1</span> - omega_0</span><br><span class="line">        mu_1 = (mu_total - mu_0) / omega_1</span><br><span class="line">        mu_0_t = mu_0 / omega_0</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 类间方差</span></span><br><span class="line">        sigma_b = omega_0 * omega_1 * (mu_0_t - mu_1) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sigma_b &gt; max_sigma:</span><br><span class="line">            max_sigma = sigma_b</span><br><span class="line">            best_thresh = t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_thresh</span><br></pre></td></tr></table></figure><hr><p>参考：<br><a href="https://www.bilibili.com/video/BV1cM8we3EVf?vd_source=59c9078be8272e804fcbbcf1da9a9f94">10.8 大津法（OTSU）（数字图像处理，冈萨雷斯版）</a></p>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD分解</title>
      <link href="/posts/undefined/"/>
      <url>/posts/undefined/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/undefined/"/>
      <url>/posts/undefined/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
